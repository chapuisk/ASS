
@article{chen_bibliometric_2018,
	title = {A bibliometric analysis of natural language processing in medical research},
	abstract = {Background: Natural language processing ({NLP}) has become an increasingly significant role in advancing medicine. Rich research achievements of {NLP} methods and applications for medical information processing are available. It is of great significance to conduct a deep analysis to understand the recent development of {NLP}-empowered medical research field. However, limited study examining the research status of this field could be found. Therefore, this study aims to quantitatively assess the academic output of {NLP} in medical research field.
Methods: We conducted a bibliometric analysis on {NLP}-empowered medical research publications retrieved from {PubMed} in the period 2007–2016. The analysis focused on three aspects. Firstly, the literature distribution characteristics were obtained with a statistics analysis method. Secondly, a network analysis method was used to reveal scientific collaboration relations. Finally, thematic discovery and evolution was reflected using an affinity propagation clustering method.
Results: There were 1405 {NLP}-empowered medical research publications published during the 10 years with an average annual growth rate of 18.39\%. 10 most productive publication sources together contributed more than 50\% of the total publications. The {USA} had the highest number of publications. A moderately significant correlation between country’s publications and {GDP} per capita was revealed. Denny, Joshua C was the most productive author. Mayo Clinic was the most productive affiliation. The annual co-affiliation and co-country rates reached 64.04\% and 15.79\% in 2016, respectively. 10 main great thematic areas were identified including Computational biology, Terminology mining, Information extraction, Text classification, Social medium as data source, Information retrieval, etc.
Conclusions: A bibliometric analysis of {NLP}-empowered medical research publications for uncovering the recent research status is presented. The results can assist relevant researchers, especially newcomers in understanding the research development systematically, seeking scientific cooperation partners, optimizing research topic choices and monitoring new scientific or technological activities.},
	pages = {14},
	author = {Chen, Xieling and Xie, Haoran and Wang, Fu Lee and Liu, Ziqing and Xu, Juan and Hao, Tianyong},
	date = {2018},
	langid = {english},
	file = {Chen et al. - 2018 - A bibliometric analysis of natural language proces.pdf:/Users/kevinchapuis/Zotero/storage/8RWFQGJW/Chen et al. - 2018 - A bibliometric analysis of natural language proces.pdf:application/pdf}
}

@article{westergaard_comprehensive_2018,
	title = {A comprehensive and quantitative comparison of text-mining in 15 million full-text articles versus their corresponding abstracts},
	volume = {14},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005962},
	doi = {10.1371/journal.pcbi.1005962},
	pages = {e1005962},
	number = {2},
	journaltitle = {{PLOS} Computational Biology},
	author = {Westergaard, David and Stærfeldt, Hans-Henrik and Tønsberg, Christian and Jensen, Lars Juhl and Brunak, Søren},
	editor = {Rzhetsky, Andrey},
	urldate = {2019-01-07},
	date = {2018-02-15},
	langid = {english},
	file = {Westergaard et al. - 2018 - A comprehensive and quantitative comparison of tex.pdf:/Users/kevinchapuis/Zotero/storage/JE5JYRBF/Westergaard et al. - 2018 - A comprehensive and quantitative comparison of tex.pdf:application/pdf}
}

@article{jinha_article_2010,
	title = {Article 50 million: an estimate of the number of scholarly articles in existence},
	volume = {23},
	issn = {09531513, 17414857},
	url = {http://doi.wiley.com/10.1087/20100308},
	doi = {10.1087/20100308},
	shorttitle = {Article 50 million},
	abstract = {How many scholarly research articles are there in existence? Journal articles first appeared in 1665,and the cumulative total is estimated here to have passed 50 million in 2009. This sum was arrived at based on published figures for global annual output for 2006, and analyses of annual output and growth rates published in the last decade.},
	pages = {258--263},
	number = {3},
	journaltitle = {Learned Publishing},
	author = {Jinha, Arif E.},
	urldate = {2019-01-07},
	date = {2010-07-01},
	langid = {english},
	file = {Jinha - 2010 - Article 50 million an estimate of the number of s.pdf:/Users/kevinchapuis/Zotero/storage/LCMTW846/Jinha - 2010 - Article 50 million an estimate of the number of s.pdf:application/pdf}
}

@inproceedings{atanassova_information_2017,
	title = {Information retrieval and semantic annotation of scientific corpora},
	url = {http://infoz.ffzg.hr/INFuture/2017/images/papers/1-03_Atanasssova,_IR_and_SA_of_Scientific_Corpora.pdf},
	doi = {10.17234/INFUTURE.2017.3},
	abstract = {Scientific papers are highly structured texts and display specific properties related to their references but also argumentative and rhetorical structure. Natural Language Processing can be applied to efficiently explore scientific corpora and develop applications for the Semantic Web, Information Retrieval, Automatic Summarization and Bibliometrics. The organization of scientific papers typically follows a standardized pattern, the well-known {IMRaD} structure (Introduction, Methods, Results, and Discussion). By analysing the full text of about 80,000 papers of the {PLOS} corpus, we studied this structure from several different perspectives. Firstly, we performed quantitative and qualitative analyses of citations and their positions in the structure of papers. Secondly, we studied the occurrences of verbs in citation contexts and their similarities across the different sections. Finally, using sentence-based similarity metrics, we quantified the phenomenon of text re-use in abstracts with respect to the {IMRaD} structure. This research allowed us to establish some of the invariants of scientific papers and the results are useful for implementing novel text mining and information retrieval interfaces taking into consideration the argumentative structure of papers. More specifically, they can be considered as an important element when creating linguistic resources and rule-based methods to perform fine-grained semantic analysis of scientific papers.},
	eventtitle = {{INFuture}2017: Integrating {ICT} in Society},
	pages = {21--26},
	booktitle = {Integrating {ICT} in Society},
	publisher = {Department of Information and Communication Sciences, Faculty of Humanities and Social Sciences, University of Zagreb, Croatia},
	author = {Atanassova, Iana},
	urldate = {2019-04-03},
	date = {2017},
	langid = {english},
	file = {Atanassova - 2017 - Information retrieval and semantic annotation of s.pdf:/Users/kevinchapuis/Zotero/storage/2KWE5CS8/Atanassova - 2017 - Information retrieval and semantic annotation of s.pdf:application/pdf}
}

@article{gulo_mining_2015,
	title = {Mining Scientific Articles Powered by Machine Learning Techniques},
	url = {http://drops.dagstuhl.de/opus/volltexte/2015/5477/},
	doi = {10.4230/oasics.iccsw.2015.21},
	abstract = {Literature review is one of the most important phases of research. Scientists must identify the gaps and challenges about a certain area; and the scientiﬁc literature, as a result of the accumulation of knowledge, should provide enough information. The problem is where to ﬁnd the best and most important articles that guarantee to ascertain the state of the art on that speciﬁc domain. A feasible literature review consists of locating, appraising, and synthesising the best empirical evidences in the pool of available publications, guided by one or more research questions. Nevertheless, it is not assured that searching interesting articles in electronic databases will retrieve the most relevant content. Indeed, the existent search engines try to recommend articles by only looking for the occurrences of given keywords. In fact, the relevance of a paper should depend on many other factors such as adequacy to the theme, speciﬁc tools used or even the test strategy, making automatic recommendation of articles a challenging problem. Our approach allows researchers to browse huge article collections and quickly ﬁnd the appropriate publications of particular interest by using machine learning techniques. The proposed solution automatically classiﬁes and prioritises the relevance of scientiﬁc papers. Using previous samples manually classiﬁed by domain experts, we apply a Naive Bayes Classiﬁer to get predicted articles from real world journal repositories such as {IEEE} Xplore or {ACM} Digital. Results suggest that our model can substantially recommend, classify and rank the most relevant articles of a particular scientiﬁc ﬁeld of interest. In our experiments, we achieved 98.22\% of accuracy in recommending articles that are present in an expert classiﬁcation list, indicating a good prediction of relevance. The recommended papers are worth, at least, reading. We envisage to expand our model in order to accept user’s ﬁlters and other inputs to improve predictions.},
	journaltitle = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik {GmbH}, Wadern/Saarbruecken, Germany},
	author = {Gulo, Carlos A. S. J. and Rúbio, Thiago R. P. M. and Tabassum, Shazia and Prado, Simone G. D.},
	urldate = {2019-01-09},
	date = {2015},
	langid = {english},
	file = {Gulo et al. - 2015 - Mining Scientific Articles Powered by Machine Lear.pdf:/Users/kevinchapuis/Zotero/storage/DB55Q2QW/Gulo et al. - 2015 - Mining Scientific Articles Powered by Machine Lear.pdf:application/pdf}
}

@article{pautasso_ten_2013,
	title = {Ten Simple Rules for Writing a Literature Review},
	volume = {9},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003149},
	doi = {10.1371/journal.pcbi.1003149},
	pages = {e1003149},
	number = {7},
	journaltitle = {{PLoS} Computational Biology},
	shortjournal = {{PLoS} Comput Biol},
	author = {Pautasso, Marco},
	editor = {Bourne, Philip E.},
	urldate = {2019-06-10},
	date = {2013-07-18},
	langid = {english},
	file = {Pautasso - 2013 - Ten Simple Rules for Writing a Literature Review.pdf:/Users/kevinchapuis/Zotero/storage/R5DUXI8M/Pautasso - 2013 - Ten Simple Rules for Writing a Literature Review.pdf:application/pdf}
}

@report{noauthor_guidelines_2009,
	location = {Basel},
	title = {Guidelines for writing a review article},
	institution = {Pant Science Center},
	date = {2009},
	file = {guidelines_review_article.pdf:/Users/kevinchapuis/Zotero/storage/3Q99435Y/guidelines_review_article.pdf:application/pdf}
}

@article{boyack_characterizing_2018,
	title = {Characterizing in-text citations in scientific articles: A large-scale analysis},
	volume = {12},
	issn = {17511577},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1751157717303516},
	doi = {10.1016/j.joi.2017.11.005},
	shorttitle = {Characterizing in-text citations in scientific articles},
	abstract = {We report characteristics of in-text citations in over five million full text articles from two large databases – the {PubMed} Central Open Access subset and Elsevier journals – as functions of time, textual progression, and scientific field. The purpose of this study is to understand the characteristics of in-text citations in a detailed way prior to pursuing other studies focused on answering more substantive research questions. As such, we have analyzed in-text citations in several ways and report many findings here. Perhaps most significantly, we find that there are large field-level differences that are reflected in position within the text, citation interval (or reference age), and citation counts of references. In general, the fields of Biomedical and Health Sciences, Life and Earth Sciences, and Physical Sciences and Engineering have similar reference distributions, although they vary in their specifics. The two remaining fields, Mathematics and Computer Science and Social Science and Humanities, have different reference distributions from the other three fields and between themselves. We also show that in all fields the numbers of sentences, references, and in-text mentions per article have increased over time, and that there are field-level and temporal differences in the numbers of in-text mentions per reference. A final finding is that references mentioned only once tend to be much more highly cited than those mentioned multiple times.},
	pages = {59--73},
	number = {1},
	journaltitle = {Journal of Informetrics},
	shortjournal = {Journal of Informetrics},
	author = {Boyack, Kevin W. and van Eck, Nees Jan and Colavizza, Giovanni and Waltman, Ludo},
	urldate = {2019-06-11},
	date = {2018-02},
	langid = {english},
	file = {Boyack et al. - 2018 - Characterizing in-text citations in scientific art.pdf:/Users/kevinchapuis/Zotero/storage/R7R2A3CY/Boyack et al. - 2018 - Characterizing in-text citations in scientific art.pdf:application/pdf}
}

@article{kamkarhaghighi_global-local_nodate,
	title = {Global-Local Word Embedding for Text Classification},
	pages = {137},
	author = {Kamkarhaghighi, Mehran},
	langid = {english},
	file = {Kamkarhaghighi - Global-Local Word Embedding for Text Classificatio.pdf:/Users/kevinchapuis/Zotero/storage/5LCQC33W/Kamkarhaghighi - Global-Local Word Embedding for Text Classificatio.pdf:application/pdf}
}

@inproceedings{gupta_scientific_2017,
	location = {Republic and Canton of Geneva, Switzerland},
	title = {Scientific Article Recommendation by Using Distributed Representations of Text and Graph},
	isbn = {978-1-4503-4914-7},
	url = {https://doi.org/10.1145/3041021.3053062},
	doi = {10.1145/3041021.3053062},
	series = {{WWW} '17 Companion},
	abstract = {Scientific article recommendation problem deals with recommending similar scientific articles given a query article. It can be categorized as a content based similarity system. Recent advancements in representation learning methods have proven to be effective in modeling distributed representations in different modalities like images, languages, speech, networks etc. The distributed representations obtained using such techniques in turn can be used to calculate similarities. In this paper, we address the problem of scientific paper recommendation through a novel method which aims to combine multimodal distributed representations, which in this case are: 1. distributed representations of paper's content, and 2. distributed representation of the graph constructed from the bibliographic network. Through experiments we demonstrate that our method outperforms the state-of-the-art distributed representation methods in text and graph, by 29.6\% and 20.4\%, both in terms of precision and mean-average-precision respectively.},
	pages = {1267--1268},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Gupta, Shashank and Varma, Vasudeva},
	urldate = {2019-07-02},
	date = {2017},
	note = {event-place: Perth, Australia},
	keywords = {cca, deepwalk, doc2vec, recommendation system, representation learning}
}

@article{kim_bag--concepts:_2017,
	title = {Bag-of-concepts: Comprehending document representation through clustering words in distributed representation},
	volume = {266},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231217308962},
	doi = {10.1016/j.neucom.2017.05.046},
	shorttitle = {Bag-of-concepts},
	abstract = {Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate weighting scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others.},
	pages = {336--352},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Kim, Han Kyul and Kim, Hyunjoong and Cho, Sungzoon},
	urldate = {2019-07-02},
	date = {2017-11-29},
	keywords = {Bag-of-concepts, Interpretable document representation, Word2vec clustering},
	file = {ScienceDirect Full Text PDF:/Users/kevinchapuis/Zotero/storage/DAW3Q2LA/Kim et al. - 2017 - Bag-of-concepts Comprehending document representa.pdf:application/pdf;ScienceDirect Snapshot:/Users/kevinchapuis/Zotero/storage/C32UD4FJ/S0925231217308962.html:text/html}
}

@inproceedings{subramanian_unsupervised_2016,
	title = {Unsupervised Text Classification and Search using Word Embeddings on a Self-Organizing Map},
	doi = {10.5120/ijca2016912570},
	abstract = {This paper presents the results of an experimental implementation of a document classifier leveraging contextual word embeddings clustered on a self-organizing map. The problem of document categorization is further compounded when there are no predefined categories, or conversely there are too many categories, that documents may be bucketed into. This paper proposes to address these problems by modelling the major themes contained in the document corpus into a cluster-map using a self-organizing neural network. The cluster-map provides a visual representation to explore the corpus, and a near-semantic search interface of the many concepts outlined across the corpus.},
	author = {Subramanian, Suraj and Vora, Deepali},
	date = {2016},
	keywords = {Artificial neural network, Biological Neural Networks, Body of uterus, Categories, Document classification, Organizing (structure), Self-organization, Self-organizing map, Semantic search, Statistical classification},
	file = {Full Text PDF:/Users/kevinchapuis/Zotero/storage/SFBKBAQA/Subramanian et Vora - 2016 - Unsupervised Text Classification and Search using .pdf:application/pdf}
}

@online{noauthor_swagger_nodate,
	title = {Swagger {UI}},
	url = {https://core.ac.uk/docs/#!/articles/getArticleByCoreId},
	urldate = {2019-07-03},
	file = {Swagger UI:/Users/kevinchapuis/Zotero/storage/6SZR7FYU/docs.html:text/html}
}

@article{lockwood_systematic_2017,
	title = {Systematic reviews: Guidelines, tools and checklists for authors: Systematic review publication standards},
	volume = {19},
	issn = {14410745},
	url = {http://doi.wiley.com/10.1111/nhs.12353},
	doi = {10.1111/nhs.12353},
	shorttitle = {Systematic reviews},
	pages = {273--277},
	number = {3},
	journaltitle = {Nursing \& Health Sciences},
	shortjournal = {Nursing \& Health Sciences},
	author = {Lockwood, Craig and Oh, Eui Geum},
	urldate = {2019-07-24},
	date = {2017-09},
	langid = {english},
	file = {Lockwood and Oh - 2017 - Systematic reviews Guidelines, tools and checklis.pdf:/Users/kevinchapuis/Zotero/storage/FZNJ6PIT/Lockwood and Oh - 2017 - Systematic reviews Guidelines, tools and checklis.pdf:application/pdf}
}

@article{koutsos_efficient_2019,
	title = {An efficient framework for conducting systematic literature reviews in agricultural sciences},
	volume = {682},
	issn = {00489697},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0048969719318972},
	doi = {10.1016/j.scitotenv.2019.04.354},
	abstract = {Systematic review has generally been accepted as an effective, more complete, repeatable, and less biased type literature review that can successfully lead to evidence-based conclusions. This study attempts to develop a framework for systematic review with guidelines on how to conduct an effective systematic review for agricultural research. Systematic reviews require more time and effort but they can be used to conduct a comprehensive literature review, identifying potentially eligible articles on primary agricultural research and answering certain focused questions. A systematic review is also conducted as an example to examine whether systematic reviews are used in agricultural sciences. It was found that in the last two decades about a third (N = 29 out of 89 or 32.5\%) of the eligible studies, classiﬁed as reviews related to agricultural research, are available as free full-text from publisher, while only eighteen of them were ﬁnally eligible to be included in this systematic review.},
	pages = {106--117},
	journaltitle = {Science of The Total Environment},
	shortjournal = {Science of The Total Environment},
	author = {Koutsos, Thomas M. and Menexes, Georgios C. and Dordas, Christos A.},
	urldate = {2019-07-24},
	date = {2019-09},
	langid = {english},
	file = {Koutsos et al. - 2019 - An efficient framework for conducting systematic l.pdf:/Users/kevinchapuis/Zotero/storage/7Q63ICJN/Koutsos et al. - 2019 - An efficient framework for conducting systematic l.pdf:application/pdf}
}

@incollection{higgins_cochrane_2008,
	location = {Chichester, {UK}},
	title = {Cochrane Handbook for Systematic Reviews of Interventions - Front Matter},
	isbn = {978-0-470-71218-4 978-0-470-69951-5},
	url = {http://doi.wiley.com/10.1002/9780470712184.fmatter},
	pages = {i--xxi},
	booktitle = {Cochrane Handbook for Systematic Reviews of Interventions},
	publisher = {John Wiley \& Sons, Ltd},
	editor = {Higgins, Julian {PT} and Green, Sally},
	urldate = {2019-07-24},
	date = {2008-09-26},
	langid = {english},
	doi = {10.1002/9780470712184.fmatter},
	file = {Higgins and Green - 2008 - Front Matter.pdf:/Users/kevinchapuis/Zotero/storage/L56XLVH7/Higgins and Green - 2008 - Front Matter.pdf:application/pdf}
}

@article{van_der_mierden_software_2019,
	title = {Software tools for literature screening in systematic reviews in biomedical research},
	issn = {1868596X},
	url = {https://www.altex.org/index.php/altex/article/view/1257},
	doi = {10.14573/altex.1902131},
	abstract = {Systematic reviews ({SRs}) hold promise for implementing the 3Rs in animal sciences: they can retrieve available alternative models, help refine experiments, and identify insufficiencies in, or an excess of, scientific knowledge on a particular topic. Unfortunately, {SRs} can be labor- and time-intensive, especially the reference screening and data extraction phases. Fortunately, several software tools are available that make screening faster and easier. However, it is not always clear which features each tool offers. Therefore, a feature analysis was performed to compare different reference screening tools as objectively as possible. This analysis enables researchers to select the tool that is most appropriate for their needs.},
	journaltitle = {{ALTEX}},
	shortjournal = {{ALTEX}},
	author = {van der Mierden, Stevie},
	urldate = {2019-07-24},
	date = {2019},
	langid = {english},
	file = {van der Mierden - 2019 - Software tools for literature screening in systema.pdf:/Users/kevinchapuis/Zotero/storage/WSDGTH7K/van der Mierden - 2019 - Software tools for literature screening in systema.pdf:application/pdf}
}

@article{thomas_applications_2011,
	title = {Applications of text mining within systematic reviews},
	volume = {2},
	issn = {17592879},
	url = {http://doi.wiley.com/10.1002/jrsm.27},
	doi = {10.1002/jrsm.27},
	abstract = {{ATR}. Any given systematic review will be focused on one or more topic areas that can be encapsulated by a network of concepts and associations between them. These concepts are realized in text as technical terms, whose main purpose, unlike general language words, is to classify specialized knowledge. {ATR} (Ananiadou and {McNaught}, 2006) is the technique which automatically identiﬁes and extracts terms from text. These typically correspond to domain concepts which are found in thesauri, controlled vocabularies, and ontologies.},
	pages = {1--14},
	number = {1},
	journaltitle = {Research Synthesis Methods},
	shortjournal = {Res. Syn. Meth.},
	author = {Thomas, James and {McNaught}, John and Ananiadou, Sophia},
	urldate = {2019-07-24},
	date = {2011-03},
	langid = {english},
	file = {Thomas et al. - 2011 - Applications of text mining within systematic revi.pdf:/Users/kevinchapuis/Zotero/storage/GXHG8D5T/Thomas et al. - 2011 - Applications of text mining within systematic revi.pdf:application/pdf}
}

@online{noauthor_research_nodate,
	title = {Research Synthesis Methods - Wiley Online Library},
	url = {https://onlinelibrary.wiley.com/journal/17592887},
	urldate = {2019-07-24},
	langid = {english},
	file = {Snapshot:/Users/kevinchapuis/Zotero/storage/NF6XEQV7/17592887.html:text/html}
}

@online{noauthor_home_nodate,
	title = {Home – Iris.ai - Your Science Assistant},
	url = {https://iris.ai/},
	urldate = {2019-11-14},
	file = {Home – Iris.ai - Your Science Assistant:/Users/kevinchapuis/Zotero/storage/ASVN8C5R/iris.ai.html:text/html}
}

@article{li_poqaa:_2018,
	title = {{PoQaa}: Text Mining and Knowledge Sharing for Scientific Publications},
	abstract = {With the arising of popular repositories like {arXiv}.org, open access publication has become a trend. Publishing becomes easy. While everyone can access research papers from anywhere with a click of a button, new issues emerge. Among the exorbitant number of papers flooding into the Internet everyday, how can we know which one we shall spend time on? For the vast majority of papers that we will never be able to touch upon ourselves, how can we quickly grasp the general idea, in order to keep up with the research trend? When newcomers have questions on a research paper, who is able to help them better understand the work and pinpoint the important followup work. We present {PoQaa} (Paper oriented Question, Answer \& Announcement) to address the above challenges. It features the following main functionalities, among others: (1) Feeding popular papers to each reader based on content analysis and user interest (2) Extracting knowledge from massive corpus to provide an up-to-date bird’s eye view of the research landscape (3) Mining high-quality concepts and suggesting query terms (4) Hosting a paper centric discussion platform to enable knowledge sharing among readers and authors. We will further integrate new text mining algorithms in {PoQaa} such as “technology roadmap” and hope it can motivate new development along this direction in order to spread scientific results much faster.},
	pages = {2},
	author = {Li, Keqian and Zhang, Ping and Liu, Honglei and Zha, Hanwen and Yan, Xifeng},
	date = {2018},
	langid = {english},
	file = {Li et al. - PoQaa Text Mining and Knowledge Sharing for Scien.pdf:/Users/kevinchapuis/Zotero/storage/IXX5HXPJ/Li et al. - PoQaa Text Mining and Knowledge Sharing for Scien.pdf:application/pdf}
}

@article{c._slamet_web_nodate,
	title = {Web Scraping and Naïve Bayes Classification for Job Search Engine},
	doi = {10.1088/1757-899X/288/1/012038},
	abstract = {Many organisations (government of non-government) use websites to share
information of new recruitment for the workers. This information overflows on
thousands of sites with various attributes and criteria. However, this availability forms
a complex puzzle in the selection process and lead to inefficient runtime. This study
proposes a simple method for job searching simplification through a construction and
collaboration of web scraping technique and classification using Naïve Bayes on
search engine. This study is resulting an effective and efficient application for users to
seek a potential job that fit in with their interests.},
	author = {C. Slamet, R. Andrian and D. S. Maylawati, Suhendar and W. Darmalaksana, M A Ramdhani}
}

@article{j_difficulties_2017,
	title = {difficulties of systematic reviews},
	issn = {0888-8892},
	url = {http://agris.fao.org/agris-search/search.do?recordID=US201700287567},
	journaltitle = {Conservation biology},
	author = {J, Martin and Westgate and B, David and Lindenmayer},
	urldate = {2019-11-28},
	date = {2017},
	file = {Snapshot:/Users/kevinchapuis/Zotero/storage/TIBSMSX8/search.html:text/html}
}

@online{noauthor_watson_nodate,
	title = {Watson Explorer - Overview {\textbar} {IBM}},
	url = {https://www.ibm.com/products/watson-explorer},
	urldate = {2019-11-28},
	file = {Watson Explorer - Overview | IBM:/Users/kevinchapuis/Zotero/storage/795CQWIJ/watson-explorer.html:text/html}
}

@report{schryen_theory_2015,
	title = {Theory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of {IS} Literature},
	url = {https://econpapers.repec.org/paper/darwpaper/75207.htm},
	shorttitle = {Theory of Knowledge for Literature Reviews},
	institution = {Darmstadt Technical University, Department of Business Administration, Economics and Law, Institute for Business Studies ({BWL})},
	type = {Publications of Darmstadt Technical University, Institute for Business Studies ({BWL})},
	author = {Schryen, Guido and Wagner, Gerit and Benlian, Alexander},
	urldate = {2019-11-28},
	date = {2015},
	file = {RePEc Snapshot:/Users/kevinchapuis/Zotero/storage/H29VHMK7/75207.html:text/html;Schryen et al. - 2015 - Theory of Knowledge for Literature Reviews An Epi.pdf:/Users/kevinchapuis/Zotero/storage/8L45HMBK/Schryen et al. - 2015 - Theory of Knowledge for Literature Reviews An Epi.pdf:application/pdf}
}

@online{noauthor_revealing_nodate,
	title = {Revealing the landscape: Detecting trends in a scientific corpus - {IEEE} Conference Publication},
	url = {https://ieeexplore.ieee.org/abstract/document/7489317},
	urldate = {2019-11-28},
	file = {Revealing the landscape\: Detecting trends in a scientific corpus - IEEE Conference Publication:/Users/kevinchapuis/Zotero/storage/AUKEITFA/7489317.html:text/html}
}

@article{eric_rojas_process_2016,
	title = {Process mining in healthcare: A literature review},
	volume = {61},
	doi = {10.1016/j.jbi.2016.04.007},
	abstract = {Process Mining focuses on extracting knowledge from data generated and stored in corporate information systems in order to analyze executed processes. In the healthcare domain, process mining has been
used in different case studies, with promising results. Accordingly, we have conducted a literature review
of the usage of process mining in healthcare. The scope of this review covers 74 papers with associated
case studies, all of which were analyzed according to eleven main aspects, including: process and data
types; frequently posed questions; process mining techniques, perspectives and tools; methodologies;
implementation and analysis strategies; geographical analysis; and medical fields. The most commonly
used categories and emerging topics have been identified, as well as future trends, such as enhancing
Hospital Information Systems to become process-aware. This review can: (i) provide a useful overview
of the current work being undertaken in this field; (ii) help researchers to choose process mining algorithms, techniques, tools, methodologies and approaches for their own applications; and (iii) highlight
the use of process mining to improve healthcare processes.},
	pages = {224--236},
	journaltitle = {Journal of Biomedical Informatics},
	author = {Eric Rojas, Jorge Munoz-Gama and Marcos Sepúlveda, Daniel Capurro},
	date = {2016}
}

@book{helvi_kyngas_application_2020,
	title = {The Application of Content Analysis in Nursing Science Research},
	isbn = {978-3-030-30199-6},
	url = {https://www.springer.com/gp/book/9783030301989#aboutBook},
	abstract = {This book provides principles on content analysis and its application into development of nursing theory. It offers clear guidance to students, lecturers and researchers to gain a deeper understanding of the method of content analysis, its implementation into their own research and criteria of trustworthiness evaluation. The book is written in user-friendly language with provided research examples and cases, and the content is illustrated by figures and tables. The authors offer their expertise in providing a well thought through explanation of content analysis in didactical style, which will enhance university education. The book includes highly experienced researchers who have published articles on content analysis and the trustworthiness of the method with more than 10 000 citations.  
Divided into two parts, this book explores the application of content analysis into nursing science. The first part presents the philosophical position of content analysis, inductive and deductive methods of using content analysis, trustworthiness of the method, and ethical consideration of using content analysis. The second part informs on the theory development based on content analysis, conceptualization of the concepts of content analysis into generation of items and instrument development, and statistical testing of a hypothetical model. The last chapter shows a new approach to using content analysis in systematic reviews and quality evaluation of methodology within systematic review process. The book is an essential tool for nursing science, providing instruction on key methodological elements in order to provide rigorously conducted empirical research for clinical practice and nursing education.},
	pagetotal = {{VIII}, 115},
	author = {Helvi Kyngas, Kristina Mikkonen and Maria Kääriäinen},
	date = {2020}
}

@article{davide_russo_technical_2018,
	title = {Technical problem identification for supervised state of the art},
	volume = {51},
	url = {https://www.sciencedirect.com/science/article/pii/S240589631831468X},
	doi = {https://doi.org/10.1016/j.ifacol.2018.08.344},
	abstract = {This paper presents a method for extracting technical information from a patent pool. It was designed to support the construction of the state of the art of a technology or a product/process by automatically identifying the list of problems that the inventors have faced. The method is based on a strict ontology, which defines what a patent problem is, and a set of {IR} strategies, which identify all alternative ways adopted in the pool to describe problems. More in detail, the authors propose a set of syntactic dependency patterns, and lemmas in order to extract only the sentences including the information dealing with problems. The output is a coarse list of technical problems, automatically extracted without the user being an expert in the problems of the sector. An exemplary case dealing with injection molding field is proposed.},
	pages = {1341--1346},
	number = {11},
	journaltitle = {{IFAC}-{PapersOnLine}},
	author = {Davide Russo, Paolo Carrara and Giancarlo Facoetti},
	date = {2018}
}

@article{priscilla_robinson_literature_2015,
	title = {Literature reviews vs systematic reviews},
	volume = {39},
	doi = {/10.1111/1753-6405.12393},
	number = {2},
	journaltitle = {Australian and New Zealand Journal of Public Health},
	author = {Priscilla Robinson, John Lowe},
	date = {2015}
}

@incollection{salloum_using_2018,
	title = {Using Text Mining Techniques for Extracting Information from Research Articles},
	isbn = {978-3-319-67055-3},
	abstract = {Nowadays, research in text mining has become one of the widespread fields in analyzing natural language documents. The present study demonstrates a comprehensive overview about text mining and its current research status. As indicated in the literature, there is a limitation in addressing Information Extraction from research articles using Data Mining techniques. The synergy between them helps to discover different interesting text patterns in the retrieved articles. In our study, we collected, and textually analyzed through various text mining techniques, three hundred refereed journal articles in the field of mobile learning from six scientific databases, namely: Springer, Wiley, Science Direct, {SAGE}, {IEEE}, and Cambridge. The selection of the collected articles was based on the criteria that all these articles should incorporate mobile learning as the main component in the higher educational context. Experimental results indicated that Springer database represents the main source for research articles in the field of mobile education for the medical domain. Moreover, results where the similarity among topics could not be detected were due to either their interrelations or ambiguity in their meaning. Furthermore, findings showed that there was a booming increase in the number of published articles during the years 2015 through 2016. In addition, other implications and future perspectives are presented in the study.},
	pages = {373--397},
	booktitle = {Studies in Computational Intelligence},
	author = {Salloum, Said and Al-Emran, Mostafa and Monem, Azza and Shaalan, Khaled},
	date = {2018-01-01},
	doi = {10.1007/978-3-319-67056-0_18},
	file = {Full Text PDF:/Users/kevinchapuis/Zotero/storage/ZV2482T2/Salloum et al. - 2018 - Using Text Mining Techniques for Extracting Inform.pdf:application/pdf}
}

@online{pericic_why_nodate,
	title = {Why systematic reviews matter},
	url = {https://www.elsevier.com/connect/authors-update/why-systematic-reviews-matter},
	abstract = {A brief history, overview and practical guide for authors},
	titleaddon = {Elsevier Connect},
	author = {Pericic, Tina and Tanveer, Sarah},
	urldate = {2019-11-30},
	langid = {english},
	file = {Snapshot:/Users/kevinchapuis/Zotero/storage/3QYR33WS/why-systematic-reviews-matter.html:text/html}
}

@book{petticrew_systematic_2006,
	title = {Systematic Reviews in the Social Sciences: A Practical Guide},
	volume = {11},
	shorttitle = {Systematic Reviews in the Social Sciences},
	abstract = {Such diverse thinkers as Lao-Tze, Confucius, and U.S. Defense Secretary Donald Rumsfeld have all pointed out that we need to be able to tell the difference between real and assumed knowledge. The systematic review is a scientific tool that can help with this difficult task. It can help, for example, with appraising, summarising, and communicating the results and implications of otherwise unmanageable quantities of data. This book, written by two highly-respected social scientists, provides an overview of systematic literature review methods: Outlining the rationale and methods of systematic reviews; Giving worked examples from social science and other fields; Applying the practice to all social science disciplines; It requires no previous knowledge, but takes the reader through the process stage by stage; Drawing on examples from such diverse fields as psychology, criminology, education, transport, social welfare, public health, and housing and urban policy, among others. Including detailed sections on assessing the quality of both quantitative, and qualitative research; searching for evidence in the social sciences; meta-analytic and other methods of evidence synthesis; publication bias; heterogeneity; and approaches to dissemination.},
	author = {Petticrew, Mark and Roberts, Helen},
	date = {2006-01-01},
	doi = {10.1002/9780470754887}
}

@article{petticrew_systematic_2001,
	title = {Systematic reviews from astronomy to zoology: myths and misconceptions},
	volume = {322},
	rights = {© 2001 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/322/7278/98.1},
	doi = {10.1136/bmj.322.7278.98},
	shorttitle = {Systematic reviews from astronomy to zoology},
	abstract = {Systematic literature reviews are widely used as an aid to evidence based decision making. For example, reviews of randomised controlled trials are regularly used to answer questions about the effectiveness of healthcare interventions. The high profile of systematic reviews as a cornerstone of evidence based medicine, however, has led to several misconceptions about their purpose and methods. Among these is the belief that systematic reviews are applicable only to randomised controlled trials and that they are incapable of dealing with other forms of evidence, such as from non-randomised studies or qualitative research.

The systematic literature review is a method of locating, appraising, and synthesising evidence. The value of regularly updated systematic reviews in the assessment of effectiveness of healthcare interventions was dramatically illustrated by Antman and colleagues, who showed that review articles failed to mention advances in treatment identified by an updated systematic review.1

It is nearly a quarter of a century since Gene Glass coined the term “meta-analysis” to refer to the quantitative synthesis of the results of primary studies.2 The importance of making explicit efforts to limit bias in the review of literature, however, has been emphasised by social scientists at least since the 1960s.3 In recent years systematic reviews have found an important role in health services research, and the growing interest in evidence based approaches to decision making makes it likely that their use will increase. Not everybody accepts that systematic reviews are necessary or desirable, and as one moves further away from the clinical applications of systematic reviews cynicism about their utility grows. Several arguments are commonly used to reject a wider role for systematic reviews, and these arguments are often based on major misconceptions about the history, purpose, methods, and uses of systematic reviews. I have examined eight common myths about …},
	pages = {98--101},
	number = {7278},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Petticrew, Mark},
	urldate = {2019-11-30},
	date = {2001-01-13},
	langid = {english},
	pmid = {11154628},
	file = {Full Text:/Users/kevinchapuis/Zotero/storage/S838N5RQ/Petticrew - 2001 - Systematic reviews from astronomy to zoology myth.pdf:application/pdf;Snapshot:/Users/kevinchapuis/Zotero/storage/U7U4NUHU/98.html:text/html}
}

@article{omara-eves_using_2015,
	title = {Using text mining for study identification in systematic reviews: a systematic review of current approaches},
	volume = {4},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/2046-4053-4-5},
	doi = {10.1186/2046-4053-4-5},
	shorttitle = {Using text mining for study identification in systematic reviews},
	abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
	pages = {5},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Systematic Reviews},
	author = {O’Mara-Eves, Alison and Thomas, James and {McNaught}, John and Miwa, Makoto and Ananiadou, Sophia},
	urldate = {2019-11-30},
	date = {2015-01-14},
	file = {Full Text:/Users/kevinchapuis/Zotero/storage/LKJAF5ZQ/O’Mara-Eves et al. - 2015 - Using text mining for study identification in syst.pdf:application/pdf;Snapshot:/Users/kevinchapuis/Zotero/storage/ALLIN8W6/2046-4053-4-5.html:text/html}
}

@article{howard_swift-review:_2016,
	title = {{SWIFT}-Review: a text-mining workbench for systematic review},
	volume = {5},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-016-0263-z},
	doi = {10.1186/s13643-016-0263-z},
	shorttitle = {{SWIFT}-Review},
	abstract = {There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the {SWIFT}-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of {SWIFT}-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus.},
	pages = {87},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Systematic Reviews},
	author = {Howard, Brian E. and Phillips, Jason and Miller, Kyle and Tandon, Arpit and Mav, Deepak and Shah, Mihir R. and Holmgren, Stephanie and Pelch, Katherine E. and Walker, Vickie and Rooney, Andrew A. and Macleod, Malcolm and Shah, Ruchir R. and Thayer, Kristina},
	urldate = {2019-11-30},
	date = {2016-05-23},
	file = {Full Text:/Users/kevinchapuis/Zotero/storage/NUTCG4B4/Howard et al. - 2016 - SWIFT-Review a text-mining workbench for systemat.pdf:application/pdf;Snapshot:/Users/kevinchapuis/Zotero/storage/ALXN2EZW/s13643-016-0263-z.html:text/html}
}

@article{asmussen_smart_2019,
	title = {Smart literature review: a practical topic modelling approach to exploratory literature review},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0255-7},
	doi = {10.1186/s40537-019-0255-7},
	shorttitle = {Smart literature review},
	abstract = {Manual exploratory literature reviews should be a thing of the past, as technology and development of machine learning methods have matured. The learning curve for using machine learning methods is rapidly declining, enabling new possibilities for all researchers. A framework is presented on how to use topic modelling on a large collection of papers for an exploratory literature review and how that can be used for a full literature review. The aim of the paper is to enable the use of topic modelling for researchers by presenting a step-by-step framework on a case and sharing a code template. The framework consists of three steps; pre-processing, topic modelling, and post-processing, where the topic model Latent Dirichlet Allocation is used. The framework enables huge amounts of papers to be reviewed in a transparent, reliable, faster, and reproducible way.},
	pages = {93},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {Journal of Big Data},
	author = {Asmussen, Claus Boye and Møller, Charles},
	urldate = {2019-11-30},
	date = {2019-10-19},
	file = {Full Text:/Users/kevinchapuis/Zotero/storage/KXNN3NL3/Asmussen and Møller - 2019 - Smart literature review a practical topic modelli.pdf:application/pdf;Snapshot:/Users/kevinchapuis/Zotero/storage/X2ZJKJL4/s40537-019-0255-7.html:text/html}
}

@article{ouzzani_rayyanweb_2016,
	title = {Rayyan—a web and mobile app for systematic reviews},
	volume = {5},
	pages = {210},
	number = {1},
	journaltitle = {Systematic reviews},
	author = {Ouzzani, Mourad and Hammady, Hossam and Fedorowicz, Zbys and Elmagarmid, Ahmed},
	date = {2016}
}

@article{marshall_toward_2019,
	title = {Toward systematic review automation: a practical guide to using machine learning tools in research synthesis},
	volume = {8},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-019-1074-9},
	doi = {10.1186/s13643-019-1074-9},
	shorttitle = {Toward systematic review automation},
	abstract = {Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice.},
	pages = {163},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Marshall, Iain J. and Wallace, Byron C.},
	urldate = {2019-11-30},
	date = {2019-07-11},
	langid = {english},
	keywords = {Evidence synthesis, Machine learning, Natural language processing},
	file = {Springer Full Text PDF:/Users/kevinchapuis/Zotero/storage/FK5FTMEB/Marshall and Wallace - 2019 - Toward systematic review automation a practical g.pdf:application/pdf}
}

@article{przybyla_prioritising_2018,
	title = {Prioritising references for systematic reviews with {RobotAnalyst}: A user study},
	volume = {9},
	rights = {© 2018 The Authors. Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1311},
	doi = {10.1002/jrsm.1311},
	shorttitle = {Prioritising references for systematic reviews with {RobotAnalyst}},
	abstract = {Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate {RobotAnalyst}, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using {RobotAnalyst} with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95\% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. {RobotAnalyst}'s descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that {RobotAnalyst} can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.},
	pages = {470--488},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {Przybyła, Piotr and Brockmeier, Austin J. and Kontonatsios, Georgios and Pogam, Marie-Annick Le and {McNaught}, John and Elm, Erik von and Nolan, Kay and Ananiadou, Sophia},
	urldate = {2019-11-30},
	date = {2018},
	langid = {english},
	file = {Full Text PDF:/Users/kevinchapuis/Zotero/storage/RYR8YGRT/Przybyła et al. - 2018 - Prioritising references for systematic reviews wit.pdf:application/pdf;Snapshot:/Users/kevinchapuis/Zotero/storage/7DM7TAGE/jrsm.html:text/html}
}

@inproceedings{feng_text-mining_2017,
	location = {Nanjing},
	title = {Text-Mining Techniques and Tools for Systematic Literature Reviews: A Systematic Literature Review},
	isbn = {978-1-5386-3681-7},
	url = {http://ieeexplore.ieee.org/document/8305926/},
	doi = {10.1109/APSEC.2017.10},
	shorttitle = {Text-Mining Techniques and Tools for Systematic Literature Reviews},
	abstract = {Despite the importance of conducting systematic literature reviews ({SLRs}) for identifying the research gaps in software engineering ({SE}) research, {SLRs} are a complex, multi-stage, and time-consuming process if performed manually. Conducting an {SLR} in line with the guidelines and practice in the {SE} domain requires considerable effort and expertise. The objective of this {SLR} is to identify and classify text-mining techniques and tools that can help facilitate {SLR} activities. This study also investigates the adoption of text-mining ({TM}) techniques to support {SLR} in the {SE} domain. We performed a mixed search strategy to identify relevant studies published from January 1, 2004, to December 31, 2016. We shortlisted 32 papers into the ﬁnal set of relevant studies published in the {SE}, medicine and social science disciplines. The majority of the text-mining techniques attempted to support the study selection stage. Only 12 out of the 14 studies in the {SE} domain applied text-mining techniques, focusing primarily on facilitating the search and study selection stages. By learning from the experience of applying {TM} techniques in clinical medicine and social science ﬁelds, we believe that {SE} researchers can adopt appropriate {SLR} automation strategies for use in the {SE} ﬁeld.},
	eventtitle = {2017 24th Asia-Pacific Software Engineering Conference ({APSEC})},
	pages = {41--50},
	booktitle = {2017 24th Asia-Pacific Software Engineering Conference ({APSEC})},
	publisher = {{IEEE}},
	author = {Feng, Luyi and Chiam, Yin Kia and Lo, Sin Kuang},
	urldate = {2019-11-30},
	date = {2017-12},
	langid = {english},
	file = {Feng et al. - 2017 - Text-Mining Techniques and Tools for Systematic Li.pdf:/Users/kevinchapuis/Zotero/storage/S4DDYCSW/Feng et al. - 2017 - Text-Mining Techniques and Tools for Systematic Li.pdf:application/pdf}
}

@article{boussalis_text-mining_2016,
	title = {Text-mining the signals of climate change doubt},
	volume = {36},
	pages = {89--100},
	journaltitle = {Global Environmental Change},
	author = {Boussalis, Constantine and Coan, Travis G},
	date = {2016}
}

@article{ananiadou_supporting_2009,
	title = {Supporting systematic reviews using text mining},
	volume = {27},
	pages = {509--523},
	number = {4},
	journaltitle = {Social science computer review},
	author = {Ananiadou, Sophia and Rea, Brian and Okazaki, Naoaki and Procter, Rob and Thomas, James},
	date = {2009},
	file = {Full Text PDF:/Users/kevinchapuis/Zotero/storage/DRZ87542/Ananiadou et al. - 2007 - Supporting Systematic Reviews Using Text Mining.pdf:application/pdf}
}

@article{wu_literature_2019,
	title = {Literature Explorer: effective retrieval of scientific documents through nonparametric thematic topic detection},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-019-01721-7},
	doi = {10.1007/s00371-019-01721-7},
	shorttitle = {Literature Explorer},
	abstract = {Scientific researchers are facing a rapidly growing volume of literatures nowadays. While these publications offer rich and valuable information, the scale of the datasets makes it difficult for the researchers to manage and search for desired information efficiently. Literature Explorer is a new interactive visual analytics suite that facilitates the access to desired scientific literatures through mining and interactive visualisation. We propose a novel topic mining method that is able to uncover “thematic topics” from a scientific corpus. These thematic topics have an explicit semantic association to the research themes that are commonly used by human researchers in scientific fields, and hence are human interpretable. They also contribute to effective document retrieval. The visual analytics suite consists of a set of visual components that are closely coupled with the underlying thematic topic detection to support interactive document retrieval. The visual components are adequately integrated under the design rationale and goals. Evaluation results are given in both objective measurements and subjective terms through expert assessments. Comparisons are also made against the outcomes from the traditional topic modelling methods.},
	journaltitle = {The Visual Computer},
	shortjournal = {Vis Comput},
	author = {Wu, Shaopeng and Zhao, Youbing and Parvinzamir, Farzad and Ersotelos, Nikolaos Th. and Wei, Hui and Dong, Feng},
	urldate = {2019-11-30},
	date = {2019-08-02},
	langid = {english},
	keywords = {Text mining, Data visualisation, Scientific documents, Topic explorer, Topic modelling, Web application},
	file = {Springer Full Text PDF:/Users/kevinchapuis/Zotero/storage/DTFP558R/Wu et al. - 2019 - Literature Explorer effective retrieval of scient.pdf:application/pdf}
}

@article{kim_multi-co-training_2019,
	title = {Multi-co-training for document classification using various document representations: {TF}–{IDF}, {LDA}, and Doc2Vec},
	volume = {477},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025518308028},
	doi = {10.1016/j.ins.2018.10.006},
	shorttitle = {Multi-co-training for document classification using various document representations},
	abstract = {The purpose of document classification is to assign the most appropriate label to a specified document. The main challenges in document classification are insufficient label information and unstructured sparse format. A semi-supervised learning ({SSL}) approach could be an effective solution to the former problem, whereas the consideration of multiple document representation schemes can resolve the latter problem. Co-training is a popular {SSL} method that attempts to exploit various perspectives in terms of feature subsets for the same example. In this paper, we propose multi-co-training ({MCT}) for improving the performance of document classification. In order to increase the variety of feature sets for classification, we transform a document using three document representation methods: term frequency–inverse document frequency ({TF}–{IDF}) based on the bag-of-words scheme, topic distribution based on latent Dirichlet allocation ({LDA}), and neural-network-based document embedding known as document to vector (Doc2Vec). The experimental results demonstrate that the proposed {MCT} is robust to parameter changes and outperforms benchmark methods under various conditions.},
	pages = {15--29},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Kim, Donghwa and Seo, Deokseong and Cho, Suhyoun and Kang, Pilsung},
	urldate = {2019-12-02},
	date = {2019-03-01},
	langid = {english},
	keywords = {Document classification, Co-training, Doc2vec, {LDA}, Semi-supervised learning, {TF}–{IDF}},
	file = {ScienceDirect Full Text PDF:/Users/kevinchapuis/Zotero/storage/53BGHYQK/Kim et al. - 2019 - Multi-co-training for document classification usin.pdf:application/pdf;ScienceDirect Snapshot:/Users/kevinchapuis/Zotero/storage/K9M2AN7J/S0020025518308028.html:text/html}
}

@incollection{aggarwal_survey_2012,
	location = {Boston, {MA}},
	title = {A Survey of Text Clustering Algorithms},
	isbn = {978-1-4614-3222-7 978-1-4614-3223-4},
	url = {http://link.springer.com/10.1007/978-1-4614-3223-4_4},
	abstract = {Clustering is a widely studied data mining problem in the text domains.},
	pages = {77--128},
	booktitle = {Mining Text Data},
	publisher = {Springer {US}},
	author = {Aggarwal, Charu C. and Zhai, {ChengXiang}},
	editor = {Aggarwal, Charu C. and Zhai, {ChengXiang}},
	urldate = {2019-12-08},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-1-4614-3223-4_4},
	file = {Aggarwal and Zhai - 2012 - A Survey of Text Clustering Algorithms.pdf:/Users/kevinchapuis/Zotero/storage/SK4U4K6X/Aggarwal and Zhai - 2012 - A Survey of Text Clustering Algorithms.pdf:application/pdf}
}

@collection{aggarwal_mining_2012,
	location = {New York},
	title = {Mining Text Data},
	isbn = {978-1-4614-3222-7},
	url = {https://www.springer.com/gp/book/9781461432227},
	abstract = {Text mining applications have experienced tremendous advances because of web 2.0 and social networking applications. Recent advances in hardware and software technology have lead to a number of unique scenarios where text mining algorithms are learned. Mining Text Data introduces an important niche in the text analytics field, and is an edited volume contributed by leading international researchers and practitioners focused on social networks \& data mining. This book contains a wide swath in topics across social networks \& data mining. Each chapter contains a comprehensive survey including the key research content on the topic, and the future directions of research in the field. There is a special focus on Text Embedded with Heterogeneous and Multimedia Data which makes the mining process much more challenging. A number of methods have been designed such as transfer learning and cross-lingual mining for such cases. Mining Text Data simplifies the content, so that advanced-level students, practitioners and researchers in computer science can benefit from this book. Academic and corporate libraries, as well as {ACM}, {IEEE}, and Management Science focused on information security, electronic commerce, databases, data mining, machine learning, and statistics are the primary buyers for this reference book.},
	publisher = {Springer-Verlag},
	editor = {Aggarwal, Charu C. and Zhai, {ChengXiang}},
	urldate = {2019-12-09},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-1-4614-3223-4},
	file = {Snapshot:/Users/kevinchapuis/Zotero/storage/YTJFRLMD/9781461432227.html:text/html}
}

@online{noauthor_text_nodate,
	title = {Text Mining: Classification, Clustering, and Applications},
	url = {https://www.crcpress.com/Text-Mining-Classification-Clustering-and-Applications/Srivastava-Sahami/p/book/9781420059403},
	shorttitle = {Text Mining},
	abstract = {The Definitive Resource on Text Mining Theory and Applications from Foremost Researchers in the Field
Giving a broad perspective of the field from numerous vantage points, Text Mining: Classification, Clustering, and Applications focuses on statistical methods for text mining and analysis. It examin},
	titleaddon = {{CRC} Press},
	urldate = {2019-12-09},
	langid = {english},
	file = {Snapshot:/Users/kevinchapuis/Zotero/storage/C37855N8/9781420059403.html:text/html}
}

@book{aggarwal_data_2014,
	title = {Data classification: algorithms and applications},
	shorttitle = {Data classification},
	publisher = {{CRC} press},
	author = {Aggarwal, Charu C.},
	date = {2014},
	file = {Snapshot:/Users/kevinchapuis/Zotero/storage/VNKMUYS3/books.html:text/html}
}

@article{alghamdi_survey_2015,
	title = {A survey of topic modeling in text mining},
	volume = {6},
	number = {1},
	journaltitle = {Int. J. Adv. Comput. Sci. Appl.({IJACSA})},
	author = {Alghamdi, Rubayyi and Alfalqi, Khalid},
	date = {2015}
}

@article{harris_distributional_1954,
	title = {Distributional structure},
	volume = {10},
	pages = {146--162},
	number = {2},
	journaltitle = {Word},
	author = {Harris, Zellig S},
	date = {1954}
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed representations of words and phrases and their compositionality},
	pages = {3111--3119},
	booktitle = {Advances in neural information processing systems},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	date = {2013}
}

@article{mikolov_efficient_2013,
	title = {Efficient estimation of word representations in vector space},
	journaltitle = {{arXiv} preprint {arXiv}:1301.3781},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	date = {2013}
}

@article{shah_document_2012,
	title = {Document clustering: a detailed review},
	volume = {4},
	pages = {30--38},
	number = {5},
	journaltitle = {International Journal of Applied Information Systems},
	author = {Shah, Neepa and Mahajan, Sunita},
	date = {2012}
}

@article{makhzani_adversarial_2015,
	title = {Adversarial autoencoders},
	journaltitle = {{arXiv} preprint {arXiv}:1511.05644},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	date = {2015}
}

@article{vishnu_automated_2019,
	title = {Automated Text Clustering and Labeling using Hypernyms},
	volume = {14},
	abstract = {Automated text clustering and labeling is a process of dividing a corpus of documents into one (or) more cluster(s) and then choosing an appropriate label for each of the clusters which will be descriptive of that particular cluster. This lets the users or anyone wishing to use these documents get an easy and quick understanding of the documents with the help of the labels we provided. These labels are aimed at being the words which are descriptive of the words or phrases in the cluster. This process results in one or a few clusters and their label(s). So, a corpus, a collection of documents is divided into clusters using the K-Means clustering algorithm which divides the data into predefined number of clusters. It works on numbers only, so we calculate Tf-idf(Term Frequency and Inverse Document Frequency) and use this data to cluster the data using K-Means. We use Wordnet {API} to find the synonyms and then find the hypernyms. We then label the cluster based on the hypernym with highest frequency. This procedure can be used with a light or no variation for document labeling also. When these documents are required, a glance at these cluster labels must be enough to get the basic idea of the corpus under consideration. This might help a lot especially when there are a lot of documents and data to be looked at. So, in this paper of ours we describe an approach of doing so.},
	pages = {5},
	number = {2},
	author = {Vishnu, Tammishetti and Himakireeti, Konda},
	date = {2019},
	langid = {english},
	file = {Vishnu and Himakireeti - 2019 - Automated Text Clustering and Labeling using Hyper.pdf:/Users/kevinchapuis/Zotero/storage/53T47VHW/Vishnu and Himakireeti - 2019 - Automated Text Clustering and Labeling using Hyper.pdf:application/pdf}
}

@article{simon_bioreader:_2019,
	title = {{BioReader}: a text mining tool for performing classification of biomedical literature},
	volume = {19},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-019-2607-x},
	doi = {10.1186/s12859-019-2607-x},
	shorttitle = {{BioReader}},
	abstract = {Scientific data and research results are being published at an unprecedented rate. Many database curators and researchers utilize data and information from the primary literature to populate databases, form hypotheses, or as the basis for analyses or validation of results. These efforts largely rely on manual literature surveys for collection of these data, and while querying the vast amounts of literature using keywords is enabled by repositories such as {PubMed}, filtering relevant articles from such query results can be a non-trivial and highly time consuming task.},
	pages = {57},
	number = {13},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Simon, Christian and Davidsen, Kristian and Hansen, Christina and Seymour, Emily and Barnkob, Mike Bogetofte and Olsen, Lars Rønn},
	urldate = {2019-12-10},
	date = {2019-02-04},
	file = {Full Text:/Users/kevinchapuis/Zotero/storage/559UTJL8/Simon et al. - 2019 - BioReader a text mining tool for performing class.pdf:application/pdf;Snapshot:/Users/kevinchapuis/Zotero/storage/UR3KL8ZQ/s12859-019-2607-x.html:text/html}
}

@inproceedings{zhang_review_2015,
	title = {A review on text mining},
	doi = {10.1109/ICSESS.2015.7339149},
	abstract = {Because of large amounts of unstructured text data generated on the Internet, text mining is believed to have high commercial value. Text mining is the process of extracting previously unknown, understandable, potential and practical patterns or knowledge from the collection of text data. This paper introduces the research status of text mining. Then several general models are described to know text mining in the overall perspective. At last we classify text mining work as text categorization, text clustering, association rule extraction and trend analysis according to applications.},
	eventtitle = {2015 6th {IEEE} International Conference on Software Engineering and Service Science ({ICSESS})},
	pages = {681--685},
	booktitle = {2015 6th {IEEE} International Conference on Software Engineering and Service Science ({ICSESS})},
	author = {Zhang, Yu and Chen, Mengdong and Liu, Lianzhong},
	date = {2015-09},
	note = {{ISSN}: 2327-0586},
	keywords = {Internet, Text mining, association rule extraction, Association rules, data mining, Feature extraction, knowledge discovery, Knowledge discovery, Market research, pattern clustering, text analysis, text categorization, Text categorization, text clustering, text mining, trend analysis, unstructured text data},
	file = {IEEE Xplore Abstract Record:/Users/kevinchapuis/Zotero/storage/BRG6DCNT/7339149.html:text/html}
}

@incollection{mejia_systematic_2019,
	location = {Cham},
	title = {A Systematic Literature Review on Word Embeddings},
	volume = {865},
	isbn = {978-3-030-01170-3 978-3-030-01171-0},
	url = {http://link.springer.com/10.1007/978-3-030-01171-0_12},
	abstract = {This article presents a systematic literature review on word embeddings within the ﬁeld of natural language processing and text processing. A search and classiﬁcation of 140 articles on proposals of word embeddings or their application was carried out from three different sources. Word embeddings have been widely adopted with satisfactory results in natural language processing tasks in general and other domains with good results. In this paper, we report the hegemony of word embeddings based on neural models over those generated by matrix factorization (i.e., variants of word2vec). Finally, despite the good performance of word embeddings, some drawbacks and their respective solution proposals are identiﬁed, such as the lack of interpretability of the real values that make up the embedded vectors.},
	pages = {132--141},
	booktitle = {Trends and Applications in Software Engineering},
	publisher = {Springer International Publishing},
	author = {Gutiérrez, Luis and Keith, Brian},
	editor = {Mejia, Jezreel and Muñoz, Mirna and Rocha, Álvaro and Peña, Adriana and Pérez-Cisneros, Marco},
	urldate = {2019-12-11},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-01171-0_12},
	file = {Gutiérrez and Keith - 2019 - A Systematic Literature Review on Word Embeddings.pdf:/Users/kevinchapuis/Zotero/storage/GCXXEUKT/Gutiérrez and Keith - 2019 - A Systematic Literature Review on Word Embeddings.pdf:application/pdf}
}

@article{sbalchiero_topic_2020,
	title = {Topic modeling, long texts and the best number of topics. Some Problems and solutions},
	issn = {0033-5177, 1573-7845},
	url = {http://link.springer.com/10.1007/s11135-020-00976-w},
	doi = {10.1007/s11135-020-00976-w},
	abstract = {The main aim of this article is to present the results of different experiments focused on the problem of model fitting process in topic modeling and its accuracy when applied to long texts. At the same time, in fact, the digital era has made available both enormous quantities of textual data and technological advances that have facilitated the development of techniques to automate the data coding and analysis processes. In the ambit of topic modeling, different procedures were born in order to analyze larger and larger collections of texts, namely corpora, but this has posed, and continues to pose, a series of methodological questions that urgently need to be resolved. Therefore, through a series of different experiments, this article is based on the following consideration: taking into account Latent Dirichlet Allocation ({LDA}), a generative probabilistic model (Blei et al. in J Mach Learn Res 3:993–1022, 2003; Blei and Lafferty in: Srivastava, Sahami (eds) Text mining: classification, clustering, and applications, Chapman \& Hall/{CRC} Press, Cambridge, 2009; Griffiths and Steyvers in Proc Natl Acad Sci {USA} ({PNAS}), 101(Supplement 1):5228–5235, 2004), the problem of fitting model is crucial because the {LDA} algorithm demands that the number of topics is specified a priori. Needles to say, the number of topics to detect in a corpus is a parameter which affect the analysis results. Since there is a lack of experiments applied to long texts, our article tries to shed new light on the complex relationship between texts’ length and the optimal number of topics. In the conclusions, we present a clear-cut powerlaw relation between the optimal number of topics and the analyzed sample size, and we formulate it in a form of a mathematical model.},
	journaltitle = {Quality \& Quantity},
	shortjournal = {Qual Quant},
	author = {Sbalchiero, Stefano and Eder, Maciej},
	urldate = {2020-02-26},
	date = {2020-02-17},
	langid = {english},
	file = {Sbalchiero and Eder - 2020 - Topic modeling, long texts and the best number of .pdf:/Users/kevinchapuis/Zotero/storage/GRIHRA9J/Sbalchiero and Eder - 2020 - Topic modeling, long texts and the best number of .pdf:application/pdf}
}

@article{weiser_clustering_2020,
	title = {A clustering approach for topic filtering within systematic literature reviews},
	issn = {22150161},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2215016120300510},
	doi = {10.1016/j.mex.2020.100831},
	abstract = {Within a systematic literature review ({SLR}), researchers are confronted with vast amounts of articles from scientific databases, which have to be manually evaluated regarding their relevance for a certain field of observation. The evaluation and filtering phase of prevalent {SLR} methodologies is therefore time consuming and hardly expressible to the intended audience. The proposed method applies natural language processing ({NLP}) on article meta data and a k-means clustering algorithm to automatically convert large article corpora into a distribution of focal topics. This allows efficient filtering as well as objectifying the process through the discussion of the clustering results. Beyond that, it allows to quickly identify scientific communities and therefore provides an iterative perspective for the so far linear {SLR} methodology.},
	pages = {100831},
	journaltitle = {{MethodsX}},
	shortjournal = {{MethodsX}},
	author = {Weißer, Tim and Saßmannshausen, Till and Ohrndorf, Dennis and Burggräf, Peter and Wagner, Johannes},
	urldate = {2020-02-28},
	date = {2020-02},
	langid = {english},
	file = {Weißer et al. - 2020 - A clustering approach for topic filtering within s.pdf:/Users/kevinchapuis/Zotero/storage/A7T8NHI3/Weißer et al. - 2020 - A clustering approach for topic filtering within s.pdf:application/pdf}
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2020-03-04},
	date = {2017-12-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/Users/kevinchapuis/Zotero/storage/G2IF6SZE/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf}
}

@article{yang_xlnet_2020,
	title = {{XLNet}: Generalized Autoregressive Pretraining for Language Understanding},
	url = {http://arxiv.org/abs/1906.08237},
	shorttitle = {{XLNet}},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like {BERT} achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, {BERT} neglects dependency between the masked positions and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we propose {XLNet}, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of {BERT} thanks to its autoregressive formulation. Furthermore, {XLNet} integrates ideas from Transformer-{XL}, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, {XLNet} outperforms {BERT} on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1.},
	journaltitle = {{arXiv}:1906.08237 [cs]},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	urldate = {2020-03-04},
	date = {2020-01-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.08237},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:/Users/kevinchapuis/Zotero/storage/W882CHUH/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:application/pdf}
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2020-03-04},
	date = {2019-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/kevinchapuis/Zotero/storage/4QG9TT8U/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@article{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s Transformers: State-of-the-art Natural Language Processing},
	url = {http://arxiv.org/abs/1910.03771},
	shorttitle = {{HuggingFace}'s Transformers},
	abstract = {Recent advances in modern Natural Language Processing ({NLP}) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in {NLP} with the starting point for training a model on a downstream task moving from a blank speciﬁc model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider {NLP} community. In this paper, we present {HuggingFace}’s Transformers library, a library for state-of-the-art {NLP}, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a uniﬁed {API} together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream {NLP} tasks. {HuggingFace}’s Transformers library features carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, {PyTorch} and {TensorFlow}, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classiﬁcation, questions answering and language generation among others. The library has gained signiﬁcant organic traction and adoption among both the researcher and practitioner communities. We are committed at {HuggingFace} to pursue the eﬀorts to develop this toolkit with the ambition of creating the standard library for building {NLP} systems. {HuggingFace}’s Transformers library is available at https://github.com/huggingface/ transformers.},
	journaltitle = {{arXiv}:1910.03771 [cs]},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Brew, Jamie},
	urldate = {2020-03-04},
	date = {2020-02-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.03771},
	keywords = {Computer Science - Computation and Language},
	file = {Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:/Users/kevinchapuis/Zotero/storage/WVNSPDJH/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:application/pdf}
}

@article{beltagy_scibert_2019,
	title = {{SciBERT}: A Pretrained Language Model for Scientific Text},
	url = {http://arxiv.org/abs/1903.10676},
	shorttitle = {{SciBERT}},
	abstract = {Obtaining large-scale annotated data for {NLP} tasks in the scientific domain is challenging and expensive. We release {SciBERT}, a pretrained language model based on {BERT} (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. {SciBERT} leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific {NLP} tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over {BERT} and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	journaltitle = {{arXiv}:1903.10676 [cs]},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	urldate = {2020-03-06},
	date = {2019-09-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.10676},
	keywords = {Computer Science - Computation and Language},
	file = {Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf:/Users/kevinchapuis/Zotero/storage/XKMDQ7Z6/Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf:application/pdf}
}

@inproceedings{luong_effective_2015,
	location = {Lisbon, Portugal},
	title = {Effective Approaches to Attention-based Neural Machine Translation},
	url = {http://aclweb.org/anthology/D15-1166},
	doi = {10.18653/v1/D15-1166},
	eventtitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	pages = {1412--1421},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
	urldate = {2020-03-07},
	date = {2015},
	langid = {english},
	file = {Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:/Users/kevinchapuis/Zotero/storage/82UPTULM/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf}
}